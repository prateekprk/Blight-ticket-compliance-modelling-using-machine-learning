{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Michigan Data Science Team (MDST) and the Michigan Student Symposium for Interdisciplinary Statistical Sciences (MSSISS) have partnered with the City of Detroit to help solve one of the most pressing problems facing Detroit - blight. Blight violations are issued by the city to individuals who allow their properties to remain in a deteriorated condition. Every year, the city of Detroit issues millions of dollars in fines to residents and every year, many of these fines remain unpaid. Enforcing unpaid blight fines is a costly and tedious process, so the city wants to know: how can we increase blight ticket compliance?\n",
    "\n",
    "The first step in answering this question is understanding when and why a resident might fail to comply with a blight ticket. This is where predictive modeling comes in. For this assignment, your task is to predict whether a given blight ticket will be paid on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['USA'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "train_data = pd.read_csv('readonly/train.csv', engine = 'python')\n",
    "test_data = pd.read_csv('readonly/test.csv', engine = 'python')\n",
    "address = pd.read_csv('readonly/addresses.csv', engine = 'python')\n",
    "latlon = pd.read_csv('readonly/latlons.csv', engine = 'python')\n",
    "\n",
    "\n",
    "drop_list = ['violator_name', 'zip_code', 'city',\n",
    "            'inspector_name', 'violation_street_number', 'violation_street_name',\n",
    "            'violation_zip_code', 'violation_description',\n",
    "            'mailing_address_str_number', 'mailing_address_str_name',\n",
    "            'non_us_str_code', 'state',\n",
    "            'ticket_issued_date', 'hearing_date', 'grafitti_status']\n",
    "train_data.drop(drop_list, axis=1, inplace=True)\n",
    "test_data.drop(drop_list, axis=1, inplace=True)\n",
    "train_data = train_data[['ticket_id', 'agency_name', 'country', 'violation_code', 'disposition',\n",
    "        'fine_amount', 'admin_fee', 'state_fee', 'late_fee', 'discount_amount',\n",
    "        'clean_up_cost', 'judgment_amount','compliance' ]]\n",
    "\n",
    "address = address.set_index('address').join(latlon.set_index('address'), how='left')\n",
    "train_data = train_data.set_index('ticket_id').join(address.set_index('ticket_id'))\n",
    "test_data = test_data.set_index('ticket_id').join(address.set_index('ticket_id'))\n",
    "\n",
    "train_data = train_data[np.isfinite(train_data['compliance'])]\n",
    "\n",
    "train_data.lat.fillna(value = train_data['lat'].mean(), inplace=True)\n",
    "train_data.lon.fillna(value = train_data['lon'].mean(), inplace=True)\n",
    "test_data.lat.fillna(value = test_data['lat'].mean(), inplace=True)\n",
    "test_data.lon.fillna(value = test_data['lon'].mean(), inplace=True)\n",
    "\n",
    "test_data['country'].unique()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# labelling columns using label_encoder function, never do manual mapping \n",
    "'''\n",
    "le.fit(train_data['disposition'].append(test_data['disposition'], ignore_index=True))\n",
    "train_data['disposition'] = le.transform(train_data['disposition'])\n",
    "test_data['disposition'] = le.transform(test_data['disposition'])\n",
    "\n",
    "le.fit(train_data['violation_code'].append(test_data['violation_code'], ignore_index=True))\n",
    "train_data['violation_code'] = le.transform(train_data['violation_code'])\n",
    "test_data['violation_code'] = le.transform(test_data['violation_code'])\n",
    "\n",
    "le.fit(train_data['country'].append(test_data['country'], ignore_index=True))\n",
    "train_data['country'] = le.transform(train_data['country'])\n",
    "test_data['country'] = le.transform(test_data['country'])\n",
    "\n",
    "le.fit(train_data['agency_name'].append(test_data['agency_name'], ignore_index=True))\n",
    "train_data['agency_name'] = le.transform(train_data['agency_name'])\n",
    "test_data['agency_name'] = le.transform(test_data['agency_name'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score is:  0.550241362514\n",
      "Confusion Matrix is:  [[37060    18]\n",
      " [ 2600   292]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "############################ Baseline Score #################################\n",
    "\n",
    "#nnn.fillna(method='pad', inplace=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = train_data.drop(['compliance'], axis = 1).values\n",
    "y = train_data.loc[:,'compliance'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "nb = GaussianNB()\n",
    "nbclf = nb.fit(X_train, y_train)\n",
    "y_pred = nbclf.predict(X_test)\n",
    "score = roc_auc_score(y_test, y_pred)\n",
    "print('score is: ', score)\n",
    "print('Confusion Matrix is: ', confusion_matrix(y_test, y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid best parameter (max. AUC):  {'max_depth': 30, 'n_estimators': 100}\n",
      "Grid best score (AUC):  0.790284023186\n"
     ]
    }
   ],
   "source": [
    "           ######################################## Main Regression Code ######################################\n",
    "'''\n",
    "clf = RandomForestRegressor()\n",
    "grid_values = {'n_estimators': [10, 100], 'max_depth': [None, 30]}\n",
    "clf_clf = GridSearchCV(clf, param_grid=grid_values, scoring='roc_auc')\n",
    "clf_clf.fit(X_train, y_train)\n",
    "\n",
    "print('Grid best parameter (max. AUC): ', clf_clf.best_params_)\n",
    "print('Grid best score (AUC): ', clf_clf.best_score_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blight_model():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "\n",
    "    #train_data = pd.read_csv('readonly/train.csv', engine = 'python')\n",
    "    train_data = pd.read_csv('train.csv', engine = 'python')\n",
    "    #test_data = pd.read_csv('readonly/test.csv', engine = 'python')\n",
    "    test_data = pd.read_csv('test.csv', engine = 'python')\n",
    "    #address = pd.read_csv('readonly/addresses.csv', engine = 'python')\n",
    "    address = pd.read_csv('addresses.csv', engine = 'python')\n",
    "    #latlon = pd.read_csv('readonly/latlons.csv', engine = 'python')\n",
    "    latlon = pd.read_csv('latlons.csv', engine = 'python')\n",
    "\n",
    "\n",
    "    drop_list = ['violator_name', 'zip_code', 'city',\n",
    "                'inspector_name', 'violation_street_number', 'violation_street_name',\n",
    "                'violation_zip_code', 'violation_description',\n",
    "                'mailing_address_str_number', 'mailing_address_str_name',\n",
    "                'non_us_str_code', 'state',\n",
    "                'ticket_issued_date', 'hearing_date', 'grafitti_status']\n",
    "    train_data.drop(drop_list, axis=1, inplace=True)\n",
    "    test_data.drop(drop_list, axis=1, inplace=True)\n",
    "    train_data = train_data[['ticket_id', 'agency_name', 'country', 'violation_code', 'disposition',\n",
    "            'fine_amount', 'admin_fee', 'state_fee', 'late_fee', 'discount_amount',\n",
    "            'clean_up_cost', 'judgment_amount','compliance' ]]\n",
    "\n",
    "    address = address.set_index('address').join(latlon.set_index('address'), how='left')\n",
    "    train_data = train_data.set_index('ticket_id').join(address.set_index('ticket_id'))\n",
    "    test_data = test_data.set_index('ticket_id').join(address.set_index('ticket_id'))\n",
    "\n",
    "    train_data = train_data[np.isfinite(train_data['compliance'])]\n",
    "\n",
    "    train_data.lat.fillna(value = train_data['lat'].mean(), inplace=True)\n",
    "    train_data.lon.fillna(value = train_data['lon'].mean(), inplace=True)\n",
    "    test_data.lat.fillna(value = test_data['lat'].mean(), inplace=True)\n",
    "    test_data.lon.fillna(value = test_data['lon'].mean(), inplace=True)\n",
    "\n",
    "    le.fit(train_data['disposition'].append(test_data['disposition'], ignore_index=True))\n",
    "    train_data['disposition'] = le.transform(train_data['disposition'])\n",
    "    test_data['disposition'] = le.transform(test_data['disposition'])\n",
    "\n",
    "    le.fit(train_data['violation_code'].append(test_data['violation_code'], ignore_index=True))\n",
    "    train_data['violation_code'] = le.transform(train_data['violation_code'])\n",
    "    test_data['violation_code'] = le.transform(test_data['violation_code'])\n",
    "\n",
    "    le.fit(train_data['country'].append(test_data['country'], ignore_index=True))\n",
    "    train_data['country'] = le.transform(train_data['country'])\n",
    "    test_data['country'] = le.transform(test_data['country'])\n",
    "\n",
    "    le.fit(train_data['agency_name'].append(test_data['agency_name'], ignore_index=True))\n",
    "    train_data['agency_name'] = le.transform(train_data['agency_name'])\n",
    "    test_data['agency_name'] = le.transform(test_data['agency_name'])\n",
    "\n",
    "    X = train_data.drop(['compliance'], axis = 1).values\n",
    "    y = train_data.loc[:,'compliance'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    clf = RandomForestRegressor()\n",
    "    grid_values = {'n_estimators': [10, 100], 'max_depth': [None, 30]}\n",
    "    clf_clf = GridSearchCV(clf, param_grid=grid_values, scoring='roc_auc')\n",
    "    clf_clf.fit(X_train, y_train)\n",
    "    \n",
    "    print('Grid best parameter (max. AUC): ', clf_clf.best_params_)\n",
    "    print('Grid best score (AUC): ', clf_clf.best_score_)\n",
    "\n",
    "    return pd.DataFrame(clf_clf.predict(test_data), test_data.index) # Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid best parameter (max. AUC):  {'max_depth': 30, 'n_estimators': 100}\n",
      "Grid best score (AUC):  0.790750078409\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticket_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>284932</th>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285362</th>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285361</th>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285338</th>\n",
       "      <td>0.020920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285346</th>\n",
       "      <td>0.060168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285345</th>\n",
       "      <td>0.030920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285347</th>\n",
       "      <td>0.128762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285342</th>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285530</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284989</th>\n",
       "      <td>0.030287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285344</th>\n",
       "      <td>0.000694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285343</th>\n",
       "      <td>0.000251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285340</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285341</th>\n",
       "      <td>0.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285349</th>\n",
       "      <td>0.050168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285348</th>\n",
       "      <td>0.030920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284991</th>\n",
       "      <td>0.020287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285532</th>\n",
       "      <td>0.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285406</th>\n",
       "      <td>0.004456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285001</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285006</th>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285405</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285337</th>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285496</th>\n",
       "      <td>0.025438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285497</th>\n",
       "      <td>0.057768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285378</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285589</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285585</th>\n",
       "      <td>0.011708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285501</th>\n",
       "      <td>0.004326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285581</th>\n",
       "      <td>0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376367</th>\n",
       "      <td>0.005052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376366</th>\n",
       "      <td>0.025387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376362</th>\n",
       "      <td>0.061861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376363</th>\n",
       "      <td>0.356488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376365</th>\n",
       "      <td>0.005052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376364</th>\n",
       "      <td>0.025387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376228</th>\n",
       "      <td>0.780212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376265</th>\n",
       "      <td>0.011232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376286</th>\n",
       "      <td>0.387299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376320</th>\n",
       "      <td>0.010453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376314</th>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376327</th>\n",
       "      <td>0.837452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376385</th>\n",
       "      <td>0.092368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376435</th>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376370</th>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376434</th>\n",
       "      <td>0.088734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376459</th>\n",
       "      <td>0.172534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376478</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376473</th>\n",
       "      <td>0.009027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376484</th>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376482</th>\n",
       "      <td>0.241071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376480</th>\n",
       "      <td>0.061167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376479</th>\n",
       "      <td>0.061167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376481</th>\n",
       "      <td>0.061167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376483</th>\n",
       "      <td>0.072991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376496</th>\n",
       "      <td>0.001981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376497</th>\n",
       "      <td>0.001981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376499</th>\n",
       "      <td>0.004231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376500</th>\n",
       "      <td>0.004231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369851</th>\n",
       "      <td>0.735000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61001 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "ticket_id          \n",
       "284932     0.000804\n",
       "285362     0.000210\n",
       "285361     0.040800\n",
       "285338     0.020920\n",
       "285346     0.060168\n",
       "285345     0.030920\n",
       "285347     0.128762\n",
       "285342     0.990000\n",
       "285530     0.000000\n",
       "284989     0.030287\n",
       "285344     0.000694\n",
       "285343     0.000251\n",
       "285340     0.000000\n",
       "285341     0.030400\n",
       "285349     0.050168\n",
       "285348     0.030920\n",
       "284991     0.020287\n",
       "285532     0.146000\n",
       "285406     0.004456\n",
       "285001     0.000000\n",
       "285006     0.020000\n",
       "285405     0.000000\n",
       "285337     0.000022\n",
       "285496     0.025438\n",
       "285497     0.057768\n",
       "285378     0.000000\n",
       "285589     0.000000\n",
       "285585     0.011708\n",
       "285501     0.004326\n",
       "285581     0.070000\n",
       "...             ...\n",
       "376367     0.005052\n",
       "376366     0.025387\n",
       "376362     0.061861\n",
       "376363     0.356488\n",
       "376365     0.005052\n",
       "376364     0.025387\n",
       "376228     0.780212\n",
       "376265     0.011232\n",
       "376286     0.387299\n",
       "376320     0.010453\n",
       "376314     0.000159\n",
       "376327     0.837452\n",
       "376385     0.092368\n",
       "376435     0.920000\n",
       "376370     0.980000\n",
       "376434     0.088734\n",
       "376459     0.172534\n",
       "376478     0.000000\n",
       "376473     0.009027\n",
       "376484     0.080000\n",
       "376482     0.241071\n",
       "376480     0.061167\n",
       "376479     0.061167\n",
       "376481     0.061167\n",
       "376483     0.072991\n",
       "376496     0.001981\n",
       "376497     0.001981\n",
       "376499     0.004231\n",
       "376500     0.004231\n",
       "369851     0.735000\n",
       "\n",
       "[61001 rows x 1 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blight_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "nNS8l",
   "launcher_item_id": "yWWk7",
   "part_id": "w8BSS"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
